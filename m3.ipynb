{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a5b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_cosine_schedule_with_warmup, GPT2ForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4ce499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e097a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2-medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3deae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 8\n",
    "dropout = 0.2\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuning model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc4c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=-1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5f8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, filename, augmentation=True):\n",
    "        with open(filename, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "            \n",
    "        if augmentation:\n",
    "            with open('data/filter_gen_dataset_mhy_math.json', 'r') as f1:\n",
    "                math_data = json.load(f1)\n",
    "            with open('data/filter_gen_dataset_mhy_openbookqa.json', 'r') as f2:\n",
    "                code_data = json.load(f2) \n",
    "            raw_data.extend(math_data)\n",
    "            raw_data.extend(code_data)\n",
    "            self.data = raw_data\n",
    "        else:\n",
    "            self.data = raw_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chat = self.data[idx]['chat']\n",
    "        instruction, demonstration = chat.rsplit('Assistant: ', 1)\n",
    "        instruction = instruction + 'Assistant: '\n",
    "            \n",
    "        return instruction, demonstration, chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5da2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = ChatDataset('data/filter_gen_dataset_mhy_train.json', augmentation=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "eval_dataset = ChatDataset('data/filter_gen_dataset_mhy_val.json', augmentation=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7304d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation\n",
    "def evaluate(eval_loader, model, device, chat_max_length):\n",
    "    model.eval()\n",
    "    eval_loss_sum = 0\n",
    "    num_eval_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_loader):\n",
    "            \n",
    "            # Tokenize the chat\n",
    "            chat = tokenizer(batch[2], return_tensors='pt', max_length=chat_max_length, padding=\"max_length\", truncation=True).input_ids.to(device) \n",
    "\n",
    "            # Count the length of instruction and demonstration\n",
    "            instruction = batch[0]\n",
    "            demonstration = batch[1]\n",
    "            instruction_tokens_num = [tokenizer(item, return_tensors='pt').input_ids.size(1) for item in instruction] \n",
    "            demonstration_tokens_num = [tokenizer(item, return_tensors='pt').input_ids.size(1) for item in demonstration]\n",
    "        \n",
    "            # Add the eos_token to the end of demonstration if the length of input is less than 1024\n",
    "            demonstration_tokens_num = [demonstration_tokens_num[0] if instruction_tokens_num[0] + demonstration_tokens_num[0] == chat_max_length else demonstration_tokens_num[0]+1] \n",
    "\n",
    "            # Get the input_ids and target_ids\n",
    "            input_ids = chat\n",
    "            target_ids = input_ids.clone() \n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Only consider the loss for the demonstration part\n",
    "            logits_demo = logits[:, instruction_tokens_num[0]-1:instruction_tokens_num[0]+demonstration_tokens_num[0]-1, :]         # [batch size, demonstration_max_length, number of classes]\n",
    "            target_ids_demo = target_ids[:, instruction_tokens_num[0]:instruction_tokens_num[0]+demonstration_tokens_num[0]]         # [batch size, demonstration_max_length]\n",
    "            batch_loss = torch.nn.functional.cross_entropy(logits_demo.permute(0, 2, 1), target_ids_demo) \n",
    "\n",
    "            eval_loss_sum += batch_loss.item()\n",
    "            num_eval_batches = step+1\n",
    "            \n",
    "    return eval_loss_sum / num_eval_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0afa3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "At step 0, the loss = 3.0088770389556885\n",
      "Validation Loss: 2.040975540701287\n",
      "At step 1000, the loss = 1.4540754556655884\n",
      "Validation Loss: 1.5534521528726528\n",
      "At step 2000, the loss = 1.1866689920425415\n",
      "Validation Loss: 1.5092301566053081\n",
      "At step 3000, the loss = 2.151099681854248\n",
      "Validation Loss: 1.49071538261139\n",
      "At step 4000, the loss = 2.1141738891601562\n",
      "Validation Loss: 1.50893768490337\n",
      "At step 5000, the loss = 1.6128333806991577\n",
      "Validation Loss: 1.5034816463693503\n",
      "Epoch: 0 | Training Loss: 1.4962821966993343 | Validation Loss: 1.5088125083367883\n",
      "Model Saved!\n",
      "epoch 2\n",
      "At step 0, the loss = 2.141450881958008\n",
      "Validation Loss: 1.5080248972074897\n",
      "At step 1000, the loss = 1.3257321119308472\n",
      "Validation Loss: 1.4496042802678168\n",
      "At step 2000, the loss = 0.9988695979118347\n",
      "Validation Loss: 1.445424346922493\n",
      "At step 3000, the loss = 1.8954966068267822\n",
      "Validation Loss: 1.437532641747627\n",
      "At step 4000, the loss = 1.9612300395965576\n",
      "Validation Loss: 1.4581819601295698\n",
      "At step 5000, the loss = 1.3282047510147095\n",
      "Validation Loss: 1.4677049754735751\n",
      "Epoch: 1 | Training Loss: 1.2715474408487402 | Validation Loss: 1.4740631943048754\n",
      "Model Saved!\n",
      "epoch 3\n",
      "At step 0, the loss = 1.825103759765625\n",
      "Validation Loss: 1.4735517007015322\n",
      "At step 1000, the loss = 1.2168734073638916\n",
      "Validation Loss: 1.444962874315099\n",
      "At step 2000, the loss = 0.8407739400863647\n",
      "Validation Loss: 1.455806219541565\n",
      "At step 3000, the loss = 1.640269160270691\n",
      "Validation Loss: 1.448759569595245\n",
      "At step 4000, the loss = 1.7907428741455078\n",
      "Validation Loss: 1.4680896029275756\n",
      "At step 5000, the loss = 1.0757746696472168\n",
      "Validation Loss: 1.4996018704025844\n",
      "Epoch: 2 | Training Loss: 1.1022529827462757 | Validation Loss: 1.505094118544626\n",
      "epoch 4\n",
      "At step 0, the loss = 1.5676259994506836\n",
      "Validation Loss: 1.5046082293011738\n",
      "At step 1000, the loss = 1.1228690147399902\n",
      "Validation Loss: 1.4896263915871146\n",
      "At step 2000, the loss = 0.7079983949661255\n",
      "Validation Loss: 1.5121552381217458\n",
      "At step 3000, the loss = 1.3995552062988281\n",
      "Validation Loss: 1.5090954519613218\n",
      "At step 4000, the loss = 1.5648659467697144\n",
      "Validation Loss: 1.5296617524895912\n",
      "At step 5000, the loss = 0.829721212387085\n",
      "Validation Loss: 1.5905993339188669\n",
      "Epoch: 3 | Training Loss: 0.9362480456639283 | Validation Loss: 1.5972905210481976\n",
      "epoch 5\n",
      "At step 0, the loss = 1.2778637409210205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAt step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m, the loss = \u001b[39m\u001b[39m{\u001b[39;00mbatch_loss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \n\u001b[0;32m---> 49\u001b[0m     eval_loss \u001b[39m=\u001b[39m evaluate(eval_loader, model, device, \u001b[39m1024\u001b[39;49m)  \n\u001b[1;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss: \u001b[39m\u001b[39m{\u001b[39;00meval_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(eval_loader, model, device, chat_max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m target_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mclone() \n\u001b[1;32m     26\u001b[0m \u001b[39m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, labels\u001b[39m=\u001b[39;49mtarget_ids)\n\u001b[1;32m     28\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     30\u001b[0m \u001b[39m# Only consider the loss for the demonstration part\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1077\u001b[0m     input_ids,\n\u001b[1;32m   1078\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1079\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1080\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1081\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1082\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1083\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1084\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1085\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1086\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1088\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    901\u001b[0m         hidden_states,\n\u001b[1;32m    902\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    903\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    904\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    905\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    906\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    907\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    908\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    391\u001b[0m     hidden_states,\n\u001b[1;32m    392\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    395\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    396\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:186\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query, key\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[0;32m--> 186\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39;49m torch\u001b[39m.\u001b[39;49mfull(\n\u001b[1;32m    187\u001b[0m         [], value\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m0.5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39m# Layer-wise attention scaling\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_by_inverse_layer_idx:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model training\n",
    "best_eval_loss = 100\n",
    "save_path = \"./models/sft_model/sft_model_gpt2_medium_with_augmentation_data\"\n",
    "\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch {epoch+1}')\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Tokenize the chat\n",
    "        chat = tokenizer(batch[2], return_tensors='pt', max_length=1024, padding=\"max_length\", truncation=True).input_ids.to(device) \n",
    "\n",
    "        # Count the length of instruction and demonstration\n",
    "        instruction = batch[0]\n",
    "        demonstration = batch[1]\n",
    "        instruction_tokens_num = [tokenizer(item, return_tensors='pt').input_ids.size(1) for item in instruction] \n",
    "        demonstration_tokens_num = [tokenizer(item, return_tensors='pt').input_ids.size(1) for item in demonstration]\n",
    "\n",
    "        # Add the eos_token to the end of demonstration if the length of input is less than 1024\n",
    "        demonstration_tokens_num = [demonstration_tokens_num[0] if instruction_tokens_num[0] + demonstration_tokens_num[0] == 1024 else demonstration_tokens_num[0]+1] \n",
    "\n",
    "        # Get the input_ids and target_ids\n",
    "        input_ids = chat\n",
    "        target_ids = input_ids.clone() \n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Only consider the loss for the demonstration part\n",
    "        logits_demo = logits[:, instruction_tokens_num[0]-1:instruction_tokens_num[0]+demonstration_tokens_num[0]-1, :]         # [batch size, demonstration_max_length, number of classes]\n",
    "        target_ids_demo = target_ids[:, instruction_tokens_num[0]:instruction_tokens_num[0]+demonstration_tokens_num[0]]         # [batch size, demonstration_max_length]\n",
    "        batch_loss = torch.nn.functional.cross_entropy(logits_demo.permute(0, 2, 1), target_ids_demo)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(f'At step {step}, the loss = {batch_loss.item()}') \n",
    "            eval_loss = evaluate(eval_loader, model, device, 1024)  \n",
    "            print(f'Validation Loss: {eval_loss}')\n",
    "\n",
    "        epoch_loss += batch_loss.item()\n",
    "        num_batches = step+1\n",
    "        \n",
    "    train_loss = epoch_loss / num_batches\n",
    "    eval_loss = evaluate(eval_loader, model, device, 1024)    \n",
    "    print(f'Epoch: {epoch} | Training Loss: {train_loss} | Validation Loss: {eval_loss}')\n",
    "    \n",
    "    \n",
    "    if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            print(\"Model Saved!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21958c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e203461",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models/sft_model/sft_model_gpt2_medium_with_augmentation_data\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379fffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: why the earth is round? \n",
      "\n",
      "Assistant: \n",
      "\n",
      "1: why the earth is round? \n",
      "\n",
      "Assistant:  The correct answer is: \"The Earth is spherical.\" This is because the Earth's shape is due to the curvature of its surface, which is caused by the gravitational pull of the sun, moon, and planets. The other options are incorrect because they do not accurately describe the shape of our planet.\n",
      "\n",
      "2: why the earth is round? \n",
      "\n",
      "Assistant:  The correct answer is: \"The Earth is spherical.\" This is because the Earth's shape is due to the curvature of its surface, which is caused by the gravitational pull of the sun, moon, and planets. The other options are incorrect because they do not accurately describe the shape of our planet.\n",
      "\n",
      "\n",
      "3: why the earth is round? \n",
      "\n",
      "Assistant:  The correct answer is: \"The Earth is spherical.\" This is because the Earth's shape is due to the curvature of its surface, which is caused by the gravitational pull of the sun, moon, and planets. The other options are incorrect because they do not accurately describe the physical properties of our planet.\n",
      "\n",
      "4: why the earth is round? \n",
      "\n",
      "Assistant:  The correct answer is: \"The Earth is spherical.\" This is because the Earth's shape is due to the curvature of its surface, which is caused by the gravitational pull of the sun, moon, and planets. The other options are incorrect because they do not accurately describe the shape of our Earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the input question\n",
    "question = \"why the earth is round? \\n\\nAssistant: \"\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate the attention mask\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "# Generate the answer\n",
    "output = model.generate(input_ids=input_ids, attention_mask = attention_mask, max_length=100, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the response\n",
    "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"Response:\", response)\n",
    "\n",
    "for i, beam in enumerate(output):\n",
    "    print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429ed968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: what the result of 2 plus 10? \n",
      "\n",
      "Assistant: \n",
      "\n",
      "1: what the result of 2 plus 10? \n",
      "\n",
      "Assistant:  \n",
      "\n",
      "2: what the result of 2 plus 10? \n",
      "\n",
      "Assistant:  The answer is $\\boxed{2+10}$. This is because we can use the fact that the sum of two numbers is equal to the product of the first number and the second number, which is the case when we multiply 2 by 10. In this case, we are multiplying 10 by 2, so we need to add 10 to get 2.\n",
      "\n",
      "3: what the result of 2 plus 10? \n",
      "\n",
      "Assistant:  The answer is $\\boxed{2+10}$. This is because we can use the fact that the sum of two numbers is equal to the product of the first number and the second number, which is the case when we multiply 2 by 10. In this case, we are multiplying 10 by 2, so we need to add 10 to get the final answer.\n",
      "\n",
      "4: what the result of 2 plus 10? \n",
      "\n",
      "Assistant:  The answer is $\\boxed{2+10}$. This is because we can use the fact that the sum of two numbers is equal to the product of the first number and the second number, which is the case when we multiply 2 by 10. In this case, we are multiplying 10 by 2, so we need to add 10 to get 2. Therefore, the total number of terms in the equation is 2 + 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the input question\n",
    "question = \"what the result of 2 plus 10? \\n\\nAssistant: \"\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate the attention mask\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "# Generate the answer\n",
    "output = model.generate(input_ids=input_ids, attention_mask = attention_mask, max_length=100, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the response\n",
    "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(\"Response:\", response)\n",
    "\n",
    "for i, beam in enumerate(output):\n",
    "    print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
