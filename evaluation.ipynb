{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I: Loading the evaluation dataset\n",
    "We will now load the provided evaluation dataset and add a helper function for preparing model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '202f9e3f-be4d-4b51-8789-f4eb0666690f',\n",
       " 'question': 'When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)',\n",
       " 'answer': ['add a regularization term', 'remove degenerate features'],\n",
       " 'choices': ['reduce learning rate',\n",
       "  'add a regularization term',\n",
       "  'remove degenerate features',\n",
       "  'add more features']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET_PATH = \"data/prompts.json\"\n",
    "with open(TEST_DATASET_PATH, \"r\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "sample_question = test_dataset[5]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please answer the following multiple choice question by selecting one of the options and justify your answer.\\nQuestion: When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)\\nOptions:\\nreduce learning rate\\nadd a regularization term\\nremove degenerate features\\nadd more features'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_model_input(datapoint):\n",
    "    # initialize the instruction and the question\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" not in datapoint or datapoint[\"choices\"] == []:\n",
    "        prepend = \"Please answer the following question and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    else:\n",
    "        prepend = \"Please answer the following multiple choice question by selecting one of the options and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"] + \"\\n\" + \"Options:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "    \n",
    "    return prepend + question_body\n",
    "\n",
    "prepare_model_input(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: Syntactic similarity scores\n",
    "As mentioned in the project plan (*Milestone 1*), we will use the following syntactic similairity measures to perform qualitative evaluation our model's performance:\n",
    "- BLEU score\n",
    "- ROUGE score\n",
    "\n",
    "Below, we define appropriate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.775353993361614e-78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python\n",
    "\n",
    "def bleu(hypothesis, reference):\n",
    "    hypothesis_tokens = [token.lower() for token in hypothesis.split()]\n",
    "    reference_tokens = [token.lower() for token in reference.split()]\n",
    "    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "bleu(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rouge(hypothesis, reference):\n",
    "    return scorer.score(hypothesis, reference)['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363636"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV: Reward model scoring\n",
    "We will also use the reward model to score the assistant model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained reward model\n",
    "REWARD_MODEL_PATH = \"models/reward_model\"\n",
    "reward_model = GPT2ForSequenceClassification.from_pretrained(REWARD_MODEL_PATH, num_labels=1).to(device)\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "reward_model.eval()\n",
    "\n",
    "reward_tokenizer = GPT2Tokenizer.from_pretrained(REWARD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1174707412719727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate reward function score for a given text input \n",
    "def calculate_reward(text):\n",
    "    inputs = reward_tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    reward = reward_model(**inputs).logits\n",
    "    return reward.item()\n",
    "\n",
    "calculate_reward(\"Hey Tom, how are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground truth examples\n",
    "Similarly to what we proposed in M2, we generate ground truth answers from the provided answers using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_wrapper\n",
    "from tqdm import tqdm\n",
    "from gpt_wrapper import APIException\n",
    "gpt_wrapper.api_key = json.load(open(\"./secrets/api_key.json\", \"r\"))\n",
    "from gpt_wrapper.chat import Chat\n",
    "\n",
    "# what should we do as system input?\n",
    "def generate_positive_sample(datapoint, print_chatgpt_input=False):\n",
    "    # annotation with ChatGPT\n",
    "    chat = gpt_wrapper.chat.Chat.create(name=\"Annotator\")\n",
    "\n",
    "    # initialize the instruction and the question\n",
    "    instruction = \"Given the question\"\n",
    "    question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" in datapoint and datapoint[\"choices\"] is not None and datapoint[\"choices\"] != []:\n",
    "        question_body += \"\\nChoices:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "        instruction += \", the choices, \"\n",
    "    \n",
    "    # add the answer to the model input\n",
    "    if isinstance(datapoint[\"answer\"], list):\n",
    "        # this is the case for multiple choice question with multiple correct options - the answer can be an array of correct options\n",
    "        answers = datapoint[\"answer\"]\n",
    "        \n",
    "        # some solutions even have a nested list as the answer, see \"sol_id\": 2296267\n",
    "        answers = [\" \".join(str(answer)) if isinstance(answer, list) else str(answer) for answer in answers]\n",
    "        \n",
    "        question_body += \"\\nAnswers:\\n\" + \"\\n\".join(answers)\n",
    "        instruction += \"and the correct answers, repeat the correct answers and produce a justification why they are correct.\"\n",
    "    else:\n",
    "        # this is the case for a question with a single answer\n",
    "        question_body += \"\\nAnswer:\\n\" + str(datapoint[\"answer\"])\n",
    "        instruction += \"and the correct answer, repeat the answer and produce a justification why the given answer is correct.\"\n",
    "\n",
    "    # if explanation provided, append it to the model input\n",
    "    if \"explanation\" in datapoint and datapoint[\"explanation\"] is not None and datapoint[\"explanation\"] != \"\":\n",
    "        question_body += \"\\nExplanation:\\n\" + datapoint[\"explanation\"]\n",
    "        instruction += \" Base your justification on the provided explanation.\"\n",
    "    \n",
    "    # for debugging purposes\n",
    "    if print_chatgpt_input:\n",
    "        print(\"Question body:\\n\", question_body, \"\\n\\n\", \"Instruction:\\n\", instruction)\n",
    "\n",
    "    try:\n",
    "        output = chat.ask(content=question_body, instruction=instruction, model_args={\"max_tokens\": 1024}).content.strip()\n",
    "    except APIException as exception:\n",
    "        exception_msg = exception.args[0]\n",
    "        print(\"Error for id: \", datapoint[\"guid\"], \": \", exception_msg)\n",
    "        output = \"\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:26<03:02,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for id:  92251192-94ec-4d07-a94c-34775db00d70 :  Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [03:01<01:20,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI rate limit hit.\n",
      "Retrying in 10 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:37<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate ground truth for each question\n",
    "ground_truths = {}\n",
    "for datapoint in tqdm(test_dataset):\n",
    "    ground_truths[datapoint[\"guid\"]] = generate_positive_sample(datapoint)\n",
    "\n",
    "with open(\"data/promts_ground_truth.json\", \"w\") as f:\n",
    "    json.dump(ground_truths, f)1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results against ground truth\n",
    "We use `gen_script_mhy.py` to generate answers to the test questions using different models (gpt-2-medium without fine tuning, gpt-2-medium fine tuned on original data, gpt-2-medium fine tuned on augmented data and the model trained with ppo). Then, we evaluate the results:\n",
    "- we compare bleu & rouge similarity scores with ground truth and compare the results of different models\n",
    "- we calculate reward model scores\n",
    "- we perform qualitative evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generated_answers(model_name):\n",
    "    answers_path = f\"answers_mhy_{model_name}.jsonl\"\n",
    "    answers = {}\n",
    "    with open(answers_path, mode='r') as f:\n",
    "        for line in f.readlines():\n",
    "            line_ = line.strip()\n",
    "            data = json.loads(line_.strip())\n",
    "            answers[data[\"guid\"]] = data[\"answer\"]\n",
    "    return answers\n",
    "\n",
    "gpt2_pretrained_answers = load_generated_answers(\"pretrained_gpt2_medium\")\n",
    "sft_answers = load_generated_answers(\"sft_model_gpt2_medium_with_original_data\")\n",
    "sft_aug_answers = load_generated_answers(\"sft_model_gpt2_medium_with_augmentation_data\")\n",
    "ppo_answers = load_generated_answers(\"ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/homebrew/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# calculate rouge and bleu scores\n",
    "def bleu_scores(answers):\n",
    "    return pd.Series([bleu(answers[guid], ground_truths[guid]) for guid in ground_truths])\n",
    "\n",
    "def rouge_scores(answers):\n",
    "    return pd.Series([rouge(answers[guid], ground_truths[guid]) for guid in ground_truths])\n",
    "\n",
    "\n",
    "ppo_bleu = bleu_scores(ppo_answers)\n",
    "sft_bleu = bleu_scores(sft_answers)\n",
    "sft_aug_bleu = bleu_scores(sft_aug_answers)\n",
    "gpt2_pretrained_bleu = bleu_scores(gpt2_pretrained_answers)\n",
    "\n",
    "ppo_rouge = rouge_scores(ppo_answers)\n",
    "sft_rouge = rouge_scores(sft_answers)\n",
    "sft_aug_rouge = rouge_scores(sft_aug_answers)\n",
    "gpt2_pretrained_rouge = rouge_scores(gpt2_pretrained_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001284177264116556,\n",
       " 0.004595554529084039,\n",
       " 0.012106199821505893,\n",
       " 0.007869554808111903)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_pretrained_bleu.mean(), sft_bleu.mean(), sft_aug_bleu.mean(), ppo_bleu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13335171461658504,\n",
       " 0.17377090792391786,\n",
       " 0.2595082035655366,\n",
       " 0.29363926572496685)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_pretrained_rouge.mean(), sft_rouge.mean(), sft_aug_rouge.mean(), ppo_rouge.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [01:48<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "def calculate_reward_scores(answers):\n",
    "    return pd.Series(answers.values()).progress_apply(calculate_reward)\n",
    "\n",
    "tqdm.pandas()\n",
    "gpt2_pretrained_reward_scores = calculate_reward_scores(gpt2_pretrained_answers)\n",
    "sft_reward_scores = calculate_reward_scores(sft_answers)\n",
    "sft_aug_reward_scores = calculate_reward_scores(sft_aug_answers)\n",
    "ppo_reward_scores = calculate_reward_scores(ppo_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6691875358678631,\n",
       " -0.7146618721872261,\n",
       " -0.08533569379258402,\n",
       " 0.15368630621851104)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_pretrained_reward_scores.mean(), sft_reward_scores.mean(), sft_aug_reward_scores.mean(), ppo_reward_scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-nlp",
   "language": "python",
   "name": "modern-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
