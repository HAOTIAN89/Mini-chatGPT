{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I: Loading the models\n",
    "First we will load the pretrained models:\n",
    "- the generative assistant model (trained in *Milestone 3*)\n",
    "- the reward model (trained in *Milestone 2*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rouge-score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.9/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (from rouge-score) (1.24.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/homebrew/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.9/site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.9/site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.9/site-packages (from nltk->rouge-score) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.9/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=286ddd64b1d143dd11bffe5b451b7d7724d5a88c9ab3c5b7460945829373ba7c\n",
      "  Stored in directory: /Users/mstyczen/Library/Caches/pip/wheels/b0/3f/ac/cc3bc304f50c77ef38d79d8e4e2684313de39af543cb4eb3da\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 rouge-score-0.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_cosine_schedule_with_warmup, GPT2ForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pre-trained assistant model\n",
    "ASSISTANT_MODEL_PATH = \"./models/sft_model/sft_model_gpt2_with_original_data\"\n",
    "assistant_tokenizer = GPT2Tokenizer.from_pretrained(ASSISTANT_MODEL_PATH)\n",
    "assistant_model = GPT2LMHeadModel.from_pretrained(ASSISTANT_MODEL_PATH).to(device)\n",
    "assistant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained reward model\n",
    "REWARD_MODEL_PATH = \"models/reward_model\"\n",
    "reward_model = GPT2ForSequenceClassification.from_pretrained(REWARD_MODEL_PATH, num_labels=1).to(device)\n",
    "reward_model.eval()\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is 1+1?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "NUM_BEAMS =  5\n",
    "NUM_RETURNED_SEQUENCES = 1\n",
    "\n",
    "def assistant_query(question):\n",
    "    # Define the input question\n",
    "    input_ids = assistant_tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate the attention mask\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    # Generate the answer\n",
    "    output = assistant_model.generate(input_ids=input_ids, attention_mask = attention_mask, max_length=MAX_LEN, num_beams=NUM_BEAMS, no_repeat_ngram_size=2, num_return_sequences=NUM_RETURNED_SEQUENCES, early_stopping=True)\n",
    "\n",
    "    # Decode and print the response\n",
    "    return assistant_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "assistant_query(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II: Loading the evaluation dataset\n",
    "We will now load the provided evaluation dataset and add a helper function for preparing model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '202f9e3f-be4d-4b51-8789-f4eb0666690f',\n",
       " 'question': 'When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)',\n",
       " 'answer': ['add a regularization term', 'remove degenerate features'],\n",
       " 'choices': ['reduce learning rate',\n",
       "  'add a regularization term',\n",
       "  'remove degenerate features',\n",
       "  'add more features']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET_PATH = \"data/prompts.json\"\n",
    "with open(TEST_DATASET_PATH, \"r\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "sample_question = test_dataset[5]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please answer the following multiple choice question by selecting one of the options and justify your answer.\\nQuestion: When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)\\nOptions:\\nreduce learning rate\\nadd a regularization term\\nremove degenerate features\\nadd more features'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_model_input(datapoint):\n",
    "    # initialize the instruction and the question\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" not in datapoint or datapoint[\"choices\"] == []:\n",
    "        prepend = \"Please answer the following question and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    else:\n",
    "        prepend = \"Please answer the following multiple choice question by selecting one of the options and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"] + \"\\n\" + \"Options:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "    \n",
    "    return prepend + question_body\n",
    "\n",
    "prepare_model_input(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: Syntactic similarity scores\n",
    "As mentioned in the project plan (*Milestone 1*), we will use the following syntactic similairity measures to perform qualitative evaluation our model's performance:\n",
    "- BLEU score\n",
    "- ROUGE score\n",
    "\n",
    "Below, we define appropriate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.775353993361614e-78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python\n",
    "\n",
    "def bleu(hypothesis, reference):\n",
    "    hypothesis_tokens = [token.lower() for token in hypothesis.split()]\n",
    "    reference_tokens = [token.lower() for token in reference.split()]\n",
    "    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "bleu(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rouge(hypothesis, reference):\n",
    "    return scorer.score(hypothesis, reference)['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.4, recall=0.3333333333333333, fmeasure=0.3636363636363636)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.5, recall=0.5, fmeasure=0.5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV: Reward model scoring\n",
    "We will also use the reward model to score the assistant model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "def calculate_reward(output):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-nlp",
   "language": "python",
   "name": "modern-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
