{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I: Loading the evaluation dataset\n",
    "We will now load the provided evaluation dataset and add a helper function for preparing model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '202f9e3f-be4d-4b51-8789-f4eb0666690f',\n",
       " 'question': 'When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)',\n",
       " 'answer': ['add a regularization term', 'remove degenerate features'],\n",
       " 'choices': ['reduce learning rate',\n",
       "  'add a regularization term',\n",
       "  'remove degenerate features',\n",
       "  'add more features']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET_PATH = \"data/prompts.json\"\n",
    "with open(TEST_DATASET_PATH, \"r\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "sample_question = test_dataset[5]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please answer the following multiple choice question by selecting one of the options and justify your answer.\\nQuestion: When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)\\nOptions:\\nreduce learning rate\\nadd a regularization term\\nremove degenerate features\\nadd more features'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_model_input(datapoint):\n",
    "    # initialize the instruction and the question\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" not in datapoint or datapoint[\"choices\"] == []:\n",
    "        prepend = \"Please answer the following question and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    else:\n",
    "        prepend = \"Please answer the following multiple choice question by selecting one of the options and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"] + \"\\n\" + \"Options:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "    \n",
    "    return prepend + question_body\n",
    "\n",
    "prepare_model_input(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: Syntactic similarity scores\n",
    "As mentioned in the project plan (*Milestone 1*), we will use the following syntactic similairity measures to perform qualitative evaluation our model's performance:\n",
    "- BLEU score\n",
    "- ROUGE score\n",
    "\n",
    "Below, we define appropriate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.775353993361614e-78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python\n",
    "\n",
    "def bleu(hypothesis, reference):\n",
    "    hypothesis_tokens = [token.lower() for token in hypothesis.split()]\n",
    "    reference_tokens = [token.lower() for token in reference.split()]\n",
    "    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "bleu(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rouge(hypothesis, reference):\n",
    "    return scorer.score(hypothesis, reference)['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363636"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV: Reward model scoring\n",
    "We will also use the reward model to score the assistant model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained reward model\n",
    "REWARD_MODEL_PATH = \"models/reward_model\"\n",
    "reward_model = GPT2ForSequenceClassification.from_pretrained(REWARD_MODEL_PATH, num_labels=1).to(device)\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "reward_model.eval()\n",
    "\n",
    "reward_tokenizer = GPT2Tokenizer.from_pretrained(REWARD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1174707412719727"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate reward function score for a given text input \n",
    "def calculate_reward(text):\n",
    "    inputs = reward_tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    reward = reward_model(**inputs).logits\n",
    "    return reward.item()\n",
    "\n",
    "calculate_reward(\"Hey Tom, how are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground truth examples\n",
    "Similarly to what we proposed in M2, we generate ground truth answers from the provided answers using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_wrapper\n",
    "from tqdm import tqdm\n",
    "from gpt_wrapper import APIException\n",
    "gpt_wrapper.api_key = json.load(open(\"./secrets/api_key.json\", \"r\"))\n",
    "from gpt_wrapper.chat import Chat\n",
    "\n",
    "# what should we do as system input?\n",
    "def generate_positive_sample(datapoint, print_chatgpt_input=False):\n",
    "    # annotation with ChatGPT\n",
    "    chat = gpt_wrapper.chat.Chat.create(name=\"Annotator\")\n",
    "\n",
    "    # initialize the instruction and the question\n",
    "    instruction = \"Given the question\"\n",
    "    question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" in datapoint and datapoint[\"choices\"] is not None and datapoint[\"choices\"] != []:\n",
    "        question_body += \"\\nChoices:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "        instruction += \", the choices, \"\n",
    "    \n",
    "    # add the answer to the model input\n",
    "    if isinstance(datapoint[\"answer\"], list):\n",
    "        # this is the case for multiple choice question with multiple correct options - the answer can be an array of correct options\n",
    "        answers = datapoint[\"answer\"]\n",
    "        \n",
    "        # some solutions even have a nested list as the answer, see \"sol_id\": 2296267\n",
    "        answers = [\" \".join(str(answer)) if isinstance(answer, list) else str(answer) for answer in answers]\n",
    "        \n",
    "        question_body += \"\\nAnswers:\\n\" + \"\\n\".join(answers)\n",
    "        instruction += \"and the correct answers, repeat the correct answers and produce a justification why they are correct.\"\n",
    "    else:\n",
    "        # this is the case for a question with a single answer\n",
    "        question_body += \"\\nAnswer:\\n\" + str(datapoint[\"answer\"])\n",
    "        instruction += \"and the correct answer, repeat the answer and produce a justification why the given answer is correct.\"\n",
    "\n",
    "    # if explanation provided, append it to the model input\n",
    "    if \"explanation\" in datapoint and datapoint[\"explanation\"] is not None and datapoint[\"explanation\"] != \"\":\n",
    "        question_body += \"\\nExplanation:\\n\" + datapoint[\"explanation\"]\n",
    "        instruction += \" Base your justification on the provided explanation.\"\n",
    "    \n",
    "    # for debugging purposes\n",
    "    if print_chatgpt_input:\n",
    "        print(\"Question body:\\n\", question_body, \"\\n\\n\", \"Instruction:\\n\", instruction)\n",
    "    \n",
    "    output = chat.ask(content=question_body, instruction=instruction, model_args={\"max_tokens\": 1024})\n",
    "    return output.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:06<10:39,  6.46s/it]"
     ]
    }
   ],
   "source": [
    "# generate ground truth for each question\n",
    "ground_truths = {}\n",
    "for datapoint in tqdm(test_dataset):\n",
    "    ground_truths[datapoint[\"guid\"]] = generate_positive_sample(datapoint)\n",
    "\n",
    "with open(\"../data/promts_ground_truth.json\", \"w\") as f:\n",
    "    json.dump(ground_truths, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-nlp",
   "language": "python",
   "name": "modern-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
