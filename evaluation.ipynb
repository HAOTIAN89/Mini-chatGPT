{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I: Loading the evaluation dataset\n",
    "We will now load the provided evaluation dataset and add a helper function for preparing model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '202f9e3f-be4d-4b51-8789-f4eb0666690f',\n",
       " 'question': 'When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)',\n",
       " 'answer': ['add a regularization term', 'remove degenerate features'],\n",
       " 'choices': ['reduce learning rate',\n",
       "  'add a regularization term',\n",
       "  'remove degenerate features',\n",
       "  'add more features']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_DATASET_PATH = \"data/prompts.json\"\n",
    "with open(TEST_DATASET_PATH, \"r\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "sample_question = test_dataset[5]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please answer the following multiple choice question by selecting one of the options and justify your answer.\\nQuestion: When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)\\nOptions:\\nreduce learning rate\\nadd a regularization term\\nremove degenerate features\\nadd more features'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_model_input(datapoint):\n",
    "    # initialize the instruction and the question\n",
    "    \n",
    "    # if a multiple choice question, add the choices to the model input\n",
    "    if \"choices\" not in datapoint or datapoint[\"choices\"] == []:\n",
    "        prepend = \"Please answer the following question and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"]\n",
    "    else:\n",
    "        prepend = \"Please answer the following multiple choice question by selecting one of the options and justify your answer.\\n\"\n",
    "        question_body = \"Question: \" + datapoint[\"question\"] + \"\\n\" + \"Options:\\n\" + \"\\n\".join(datapoint[\"choices\"])\n",
    "    \n",
    "    return prepend + question_body\n",
    "\n",
    "prepare_model_input(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: Syntactic similarity scores\n",
    "As mentioned in the project plan (*Milestone 1*), we will use the following syntactic similairity measures to perform qualitative evaluation our model's performance:\n",
    "- BLEU score\n",
    "- ROUGE score\n",
    "\n",
    "Below, we define appropriate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.775353993361614e-78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python\n",
    "\n",
    "def bleu(hypothesis, reference):\n",
    "    hypothesis_tokens = [token.lower() for token in hypothesis.split()]\n",
    "    reference_tokens = [token.lower() for token in reference.split()]\n",
    "    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "\n",
    "bleu(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rouge(hypothesis, reference):\n",
    "    return scorer.score(hypothesis, reference)['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363636"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge(\"Hey Tom, how are you doing?\", \"Hey Tom, how is it going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV: Reward model scoring\n",
    "We will also use the reward model to score the assistant model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained reward model\n",
    "REWARD_MODEL_PATH = \"models/reward_model\"\n",
    "reward_model = GPT2ForSequenceClassification.from_pretrained(REWARD_MODEL_PATH, num_labels=1).to(device)\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "reward_model.eval()\n",
    "\n",
    "reward_tokenizer = GPT2Tokenizer.from_pretrained(REWARD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1174707412719727"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate reward function score for a given text input \n",
    "def calculate_reward(text):\n",
    "    inputs = reward_tokenizer(text, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    reward = reward_model(**inputs).logits\n",
    "    return reward.item()\n",
    "\n",
    "calculate_reward(\"Hey Tom, how are you doing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-nlp",
   "language": "python",
   "name": "modern-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
